# Overview
This section contains a detailed explanation of the first phase of the competition, involving spacecraft detection. For this phase, contestants are asked to generate bounding box data around a spacecraft, given an image generated by Blender. An example input image, along with the associated bounding box are displayed below.
<img src="https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/raw/main/example_images/no_bb.png?ref_type=heads" alt="image" width="500"/>    <img src="https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/raw/main/example_images/bb.png?ref_type=heads" alt="image" width="500"/>

# Rendering Data
In order to render additional data for this phase of the competition, contestants must run the following command:
```bash
blender -b no_spacecraft.blend -E CYCLES --python render.py
```

This will generate 10 images, in a folder named `output/trial_<trial_number>`, along with a csv file containing the priviledged information regarding the bounding box for the spacecraft. The `-b` option tells blender to run headless in the background, and the `-E CYCLES` tells blender to use the `CYCLES` render engine. `--python` option instructs blender to run the following python script.
> **&#9432; NOTE:**
> If the device that is being used to render has a NVIDIA GPU with CUDA installed, you can append `-- --cycles-device CUDA` to instruct the `CYCLES` render engine to use CUDA. The `CUDA` option can additionally be replaced with `OPTIX` if optix is installed, or with `CUDA+CPU` or `OPTIX+CPU` for hybrid rendering.

> **&#9432; NOTE:**
> For faster render times, a user can remove the `-E CYCLES` option from the above command, which tells blender to use the `EEVEE` render engine. This is only possible when running locally with a Display (see [this](https://github.com/nytimes/rd-blender-docker/issues/38) issue for more details).

## Checking Bounding Boxes
A script to check bounding boxes visually (as displayed in the image above) is also provided. This script can be run using python. The user must change [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/bounding-box/test_bb.py?ref_type=heads#L3) line to direct the script to the appropriate image file, and modify [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/bounding-box/test_bb.py?ref_type=heads#L6) line to contain the appropriate information.

> **&#9432; NOTE:**
> To run this script, a python environment with `cv2` installed must be active.

# Data Format
The csv file is formatted as follows: 
```
image_number, spacecraft_label, background_label, min_x, min_y, max_x, max_y
```

For more information on the spacecraft and background image labels, see [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/data/README.md?ref_type=heads#overview-of-json-files) document. The location variables for the bounding box are represented as the pixel numbers, assuming the origin as at the top left corner with the positive x-axis going right and positive y-axis going down (standard format for images).

The images are 1280 x 1024 resolution, with a 14 degree FOV, based on the specs of the [MQ013CG-E2](https://www.ximea.com/en/products/cameras-filtered-by-sensor-types/mq013cg-e2).

# Modifying the Data Produced
## Rendering More Images
Currently, only ten images are produced in a series. To modify this, change the [`NUMBER_OF_IMAGES`](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/bounding-box/render.py?ref_type=heads#L40) parameter in the python script.

## Changing the Spacecraft
Additionally, the only spacecraft rendered is the Mars Reconnaissance Orbiter (MRO). To modify this, change the [`spacecraft_name`](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/bounding-box/render.py?ref_type=heads#L22) parameter in the python script to a different variable name from [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/data/loadable_spacecraft/names.json?ref_type=heads) file (e.g. `ACRIMSAT`).

## Camera Augmentations
To add additional camera filter augmentations (such as blur or glare), uncomment [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/bounding-box/render.py?ref_type=heads#L278) line in the python script.

## Randomizing lighting
There are two options for the lighting during this stage. The lighting can either correspond with the background, associated with data that is loaded from the [background json file](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/data/README.md?ref_type=heads#in-image-x-y-z), or can be entirely randomized (both the lighting energy and the location of the lighting source). This can be modified by changing the [`RANDOM_LIGHTING`](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blame/main/bounding-box/render.py#L38) parameter of render script.

## Adding Spacecraft and Backgrounds
For more instructions on augmenting the dataset with your own background images and spacecraft models, follow the instructions described in [this](https://gitlab-fsl.jsc.nasa.gov/stefan.d.caldararu/synthetic-imagery/-/blob/main/data/README.md?ref_type=heads) document.
