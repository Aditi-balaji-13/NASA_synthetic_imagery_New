# Pose Estimation
This document describes the pose estimation formulation for the competition being run by R5. Here, we describe _explicitly_ what all rotations and translations are in reference to, in what order these are applied, and in what reference frame the source information and resulting information is in. Additionally, we describe how the relative poses are achieved, and how the rendering process in Blender is actually conducted. This clarity will hopefully help with understanding the data generation pipeline, as well as ensure that we describe a clear formulation of the problem, that is possible to solve.

## Background
We (the R5 team) would like to be able to **inspect** an unknown spacecraft. This requires some form of relative pose to the unknown spacecraft. As such, we are hoping to be able to move around the spacecraft, take various pictures of it, and determine the relative pose throughout the sequence of images. This is exemplified in the 2D representation shown below, where the R5 Cubesat (in the future, this may be referred to as the "camera") is represented by a triangle, and the unknown spacecraft we are trying to observe is represented by a square.

<img src=./images/P2.png alt="image" width="200"/>    <img src=./images/P1.png alt="image" width="200"/>

To be able to do this, we need to provide training data that has **accurate** labels for the relative pose of the cubesat, in relation to the spacecraft. To start, we will be assuming that all movement is by the camera/cubesat, and the spacecraft remains stationary. This way, the camera is _orbiting_ the spacecraft, and reorienting itself to take an image of the spacecraft. This is equivalent, as a movement of the spacecraft and camera can be represented as a larger movement of the camera.

<img src=./images/P2.png alt="image" width="200"/>   <img src=./images/P3.png alt="image" width="200"/>

## What Happens in Blender

In blender, moving and reorienting the camera (R5 cubesat) would pose some challenges, as we need to ensure that the spacecraft is not only in the camera image, but close to the center. As a result, the solution to this issue is to _rotate_ the spacecraft, and just apply small translations to the camera, to position the spacecraft slightly off-center in the images. This has two major benefits. First, our camera is now always aligned with the global reference frame. As a result, we can just get all rotations of the spacecraft into the global reference frame, and this will be equivalent ot the rotation of the spacecraft in the camera reference frame. Additionally, we want to ensure that the center of the camera (or potentially some other arbitrary pixel) always hits the spacecraft. To do this, all we have to do is project the spacecraft vertices onto the y-z plane, and then we get the possible locations that the camera can take. 
> **&#9432; NOTE:**
> This method is also used for the bounding box portion of the competition, and could also be easily used to train a segmentation network, as you can easily distinguish the boundary of the spacecraft from the background.

<img src=./images/P2.png alt="image" width="200"/>   <img src=./images/P4.png alt="image" width="200"/> <img src=./images/P5.png alt="image" width="200"/>

The above images demonstrate this _rotation_ of the spacecraft and _translation_ of the camera. The third image in the sequence demonstrates, in 2D, how we are able to ensure that the center pixel of the camera still hits the spacecraft by only translating the camera in the range on the y-z plane (in this image, only one dimension) of the spacecraft's projection.

## Rotation and Translation Definition of the Spacecraft

### Data Format
The data format for this portion of the competition is displayed below. THis is the full data generated by this pipeline.
> **&#9432; NOTE:**
> This table is just an example. The data in the table is not accurate, and not numerically representative of true data.

<img src=./images/Capture.PNG alt="image" width="500"/>

In this table, each row corresponds to an image taken as we move around the spacecraft, and the associated data. The first two columns are the data we will _provide_ to competitors, while the remaining columns are the _labels_ for this data, which we expect competitors to be bale to predict after training. The first two columns are the image, and the corresponding Laser Range Finder (LRF) measurement for this image. The remaining data corresponds to the positional difference, as well as the orientation difference represented as a quaternion. All labels are provided _in relation to_ the base reference frame, which is the first image in a sequence. This is explicitly described in the following Base Reference Frame. Following that, a description of the Rotation and Translation reference frame and order is provided.

### Base Reference Frame

<img src=./images/P6.png alt="image" width="500"/>

The above image demonstrates the relationship between all of the images provided. Given a sequence of images, we define the _base reference frame_ as the first image in the sequence. Then, we expect all delta poses to be in relation to this base image. The other potential option is to have delta poses defined between pairs of adjacent images in a sequence. This is displayed in the lower half, but we **do not** do this, as there may be accumulated error in competitors submissions as a result of this, even with very low errors.

### Rotation and Translation Frame of Reference and Order

<img src=./images/P7.png alt="image" width="800"/>

In the above image, we describe the _translation and rotation_ order, and the reference frame for each that we expect the competitors to produce. The 2nd frame show the translation and the rotation of the R5 cubesat around the unknown spacecraft, in order to generate a new image. The old location of the camera is shown with a dotted blue line. This image was shown in the above background section. Because there is no common geometry on the spacecraft that can be identified by competitors, we define the reference frame for all rotations and translations **at the R5 cubesat**. This is shown in each of the images. Here, we are assuming this is the first and second image in a sequence, but a similar technique can be assumed for all images in the sequence. To get the _relative pose_ between the two images, we first rotate the R5 Cubesat original orientation into the current reference frame. We then translate the spacecraft _in this current reference frame_ back to the original location. This gives us the delta pose of the spacecraft between these two images. It is important that we first apply the rotation and then apply the translation, as otherwise you may end up with a different translation.

### Online Format

This competition is also formatted as an online problem, which is important to note. This means that competitors **won't** have access to all images in the sequence during processing. When they are processing the second image in the sequence, they will only have access to the first two images. Likewise, when they are processing the 10th image in the sequence, they will have access to all of the first 10 images, but not to any of the following images. Additionally, they will not be provided with the truth data labels for the previous images when processing the images. All that they will have access to during the testing phase of the competition will be the LRF measurements and the images.

## Rotation Label Generation in Blender
In this section, we explicitly define the labsel generation for the rotation of the spacecraft between images. This is slightly more involved, because in Blender we are rotating the spacecraft, and not the R5 cubesat. We start with a background and notation section, which will also be relevant to the following section on translation label generation.

### Background and Notation
The spacecraft has some base reference frame, which is designated as the 0-rotation of the spacecraft. This is defined for the mesh, and as a result is consistent each time we load the spacecraft object into blender. We do not want this rotation to be the rotation of the spacecraft in the base image for every sequence that we generate, so we must apply some initial rotation **$R_{init}$**. We record this rotation for future reference, as all future rotations can only be accessed in relation to the initial orientation of the spacecraft (i.e. the spacecraft with no rotation applied). This is demonstrated in the example below, as the transition from panel 1 to panel 2.
> **&#9432; NOTE:**
> Here, the spacecraft is now represented as a rectangle as oppose to a square to assist with the visuals for a 90 degree rotation. Additionally, we show the _spacecraft's reference frame_, as opposed to the R5 reference frame. This is in order to provide the blender generation of the rotation. We additionally describe how this will translate to the spacecrafts rotation.

<img src=./images/P8.png alt="image" width="400"/>

Additionally, there is another rotation of the spacecraft shown, for some image $i$ in the sequence (the third panel). In blender, we can easily access the current rotation of the spacecraft, which is the transition between panel 1 and panel 3. This is referred to as **$R_c$**, i.e. the current rotation. The Rotation which we would like to the is **$R_i$**, the transition from panel 2 to panel 3. This is representative of the delta rotation of the spacecraft for the ith image. This is equivalent to the rotation of the camera that we ask competitors for, as we ask them for the **current rotation of the camera relative to the original rotation**. This is described in more detail in the previous rotation and translation order section.

To compute the desired rotation from the two known rotations, we must perform the following composition, and the output the result as a quaternion for accurate orientation labels: **$R_i = R_c R_{init}^{-1}$**.

## Translation Label Generation in Blender
Additionally, it is important to note that in the Blender scene, the spacecraft is located at the origin. Because the camera reference frame is aligned with the global reference frame, this means it must have it's initial location in the (general direction of the) negative x-axis. When computing the translational difference between the ith image and the base reference frame, we are asking competitors to provide us with the **_original location with respect to the current location in the current reference frame_**. This involves both translation due to the rotation about the spacecraft, as well as the small translational movements of the camera. In the reference shown below, the original location of the spacecraft is shown as the blue triangle with a dotted outline.

<img src=./images/P9.png alt="image" width="400"/>

This initial position is again recorded in the global reference frame. The image above is the same as the one from the previous section, except we show the original position of the camera _as if it was rotating with the spacecraft_. Because the original position of the spacecraft is recorded in the global reference frame, we must rotate it with the spacecraft as in the image. Therefore, to get the desired translation of the camera, we compute it as follows: **$d = |v_i| = |R_i v_o - v_c|$**, where **$v_i$** is the vector for the translation for the ith image, **$d$** is the magnitude for this vector, **$R_i$** is the rotation matrix from the previous section, **$v_o$** is the original position of the camera, nad **$v_c$** is the current position of the camera. 

# Miscellaneous Notes

## Laser Range Finder
There was previously an assumption that the center pixel of the camera always hits the spacecraft, and in Blender the LRF measurement is provided from here. This is not realistic, and will be reconciled by post-processing noise onto the LRF measurements. This way, we just tell competitors that _some_ pixel close to the center hits the spacecraft, but we don't have to specify which one. We additionally tell them that the LRF measurement is more of a reference, not an exact measurement. Finally, we can also omit the LRF measurement in some of the data provided during the testing phase, as sometimes we may not have this information consistently.